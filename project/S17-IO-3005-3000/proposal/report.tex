\documentclass[9pt,twocolumn,twoside]{styles/osajnl}
\usepackage{fancyvrb}
\journal{i524} 

\title{Project Proposal for I524}

\author[1,*]{Abhishek Gupta}
\author[1, **]{Avadhoot Agasti}

\affil[1]{School of Informatics and Computing, Bloomington, IN 47408, U.S.A.}

\affil[*]{Corresponding authors: abhigupt@iu.edu}
\affil[**]{Corresponding authors: aagasti@iu.edu}

\dates{project-1: Data mining for a wiki url , \today}

\ociscodes{Cloud, I524}

% replace this with your url in github/gitlab
\doi{\url{https://github.com/cloudmesh/classes/blob/master/docs/source/format/report/report.pdf}}

\begin{abstract}
\end{abstract}

\setboolean{displaycopyright}{true}


\begin{document}

\maketitle

\section{Problem}

Given a wiki URL of a person, find out his details like School, Spouse, Coaches, language, alma-meter etc Typically, the wiki page has all this information available but in the free form text. We need to converting it into structured data format so that it can help us analyze the people, from the networks etc We can create a network by navigating the people mentioned in the wiki page. 

\section{Solution}

Use spark \cite{www-spark-python} to load the wiki data and create word vectors. Train it using spark ML  \cite{www-sparkml} and then use the model for analytics and prediction. The training set will use Word2Vec. Word2vec \cite{www-word2vec} is a group of related models that are used to produce word embeddings. Word2Vec is used to analyze the linguistic context of the words. In this project, we created Word2vec model using Wikipedia data. Our focus is people and organization names occurring in the Wikipedia data and to see if Word2vec can be used to understand relationship between people. Typically Wikipedia page for people and celebraties contain the entire family and friends, colleagues information. Our idea is to use Word2vec to see if using a smaller training set of known relationships whether we can derive similar relationship for anyone who has presence on Wikipedia. This mechanism can be then used to convert the data hidden in textual format to more structured data. 

\begin{center}
 \begin{tabular}{||c c||} 
 \hline
 Technology Name & Purpose  \\ [0.5ex] 
 \hline\hline
 spark \cite{www-spark-python} & data analysis  \\
 \hline
 sparkML \cite{www-sparkml} & machine learning  \\
 \hline
 python \cite{www-spark-python} & development \\
 \hline
 ansible \cite{www-ansible} & automated deployment \\
 \hline
 collectd \cite{www-collectd} & statistics collection for benchmarking \\
 \hline
\end{tabular}
\end{center}

\section{Deployment}
Solution will be deployed using Ansible \cite{www-ansible} playbook. Automated deployment should happen on two or more nodes cluster. Deployment script should install all necessary software along with the project code to the cluster nodes.

\section{Benchmarking}
Solution will use collectd \cite{www-collectd} to collect statistics. Once the solution is deployed to the cluster. We should benchmark parameters like
\begin{itemize}[noitemsep]
\item cpu
\item memory
\item throughput reads/writes
\end{itemize}
Benchmarking will be done for one or more cloud providers. The deployment scripts should be agnostic to the cloud provider. 

% Bibliography

\bibliography{references}
 
\paragraph{6. Conclusion}

Using this wiki analysis we should be able to build a network based on wiki data.

\paragraph{Acknowledgement}

We acknowledge our professor Gregor von Laszewski and all associate instructors for helping us and guiding us throughout this project.

\end{document}
